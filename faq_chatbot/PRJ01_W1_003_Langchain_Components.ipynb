{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LangChain의 개념과 주요 컴포넌트 이해\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain이란 \n",
    "\n",
    "- **LangChain**은 LLM 기반 애플리케이션 개발을 위한 프레임워크\n",
    "\n",
    "- **Chain**은 작업을 순차적으로 실행하는 파이프라인 구조를 제공\n",
    "\n",
    "- **Agent**는 자율적 의사결정이 가능한 실행 단위\n",
    "\n",
    "\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" \n",
    "            alt=\"langchain_stack\" \n",
    "            width=\"600\" \n",
    "            style=\"border: 0;\">\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 컴포넌트 \n",
    "\n",
    "- **언어 처리 기능**은 LLM/ChatModel이 중심이 되며, Prompt와 Memory로 대화를 관리\n",
    "\n",
    "- **문서 처리와 검색**은 Document Loader, Text Splitter, Embedding, Vectorstore가 담당\n",
    "\n",
    "- **모듈성**이 핵심 특징으로, 독립적인 컴포넌트들을 조합해 RAG와 같은 복잡한 시스템을 구현 가능 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치\n",
    "# uv add ipykernel python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env 파일 설정\n",
    "# OPENAI_API_KEY=your_openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 모델 (Models)\n",
    "- LLM, ChatModel 등으로 구분\n",
    "- OpenAI, Anthropic, Google 등 다양한 모델을 지원\n",
    "- 텍스트 생성, 대화, 요약 등의 작업을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAI 모델을 사용하여 대화 생성\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "# 모델에 메시지를 보내고 응답을 받기\n",
    "response = model.invoke(\"안녕하세요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답 객체(AIMessage): 메시지(content)와 메타데이터(response_metadata 등)를 포함\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답 객체의 메시지 내용 출력\n",
    "print(\"답변: \", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답 객체의 메타데이터 출력\n",
    "print(\"메타데이터: \", response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] \n",
    "\n",
    "- Google Gemini 모델을 사용하여 텍스트 생성하고 응답 객체를 확인해보세요.\n",
    "\n",
    "- 참고: https://python.langchain.com/docs/integrations/chat/google_generative_ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain-google-genai 또는 uv add langchain-google-genai\n",
    "# .env 파일에 GOOGLE_API_KEY 추가\n",
    "# Google GenAI 모델을 사용하여 대화 생성\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 모델에 메시지를 보내고 응답을 받기\n",
    "response = gemini.invoke(\"안녕하세요!\")\n",
    "\n",
    "# 응답 객체(AIMessage): 메시지(content)와 메타데이터(response_metadata 등)를 포함\n",
    "print(\"답변: \", response.content)\n",
    "\n",
    "# 응답 객체의 메타데이터 출력\n",
    "print(\"메타데이터: \", response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 메시지 (Messages)\n",
    "- Chat Model에서 사용할 수 있는 통합된 메시지 형식을 제공\n",
    "- 각 모델 제공자의 특정 메시지 형식을 신경 쓰지 않고도 다양한 채팅 모델을 활용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. HumanMessage`\n",
    "- 사용자 역할에 해당 (user, human 등)\n",
    "- 사용자의 입력을 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 사용자 메시지 생성\n",
    "human_message = HumanMessage(content=\"Glory를 한국어로 번역해주세요.\")\n",
    "\n",
    "# 번역 요청 및 응답 받기\n",
    "response = model.invoke([human_message])  # 메시지 리스트로 전달\n",
    "\n",
    "# 답변 출력\n",
    "print(\"답변: \", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열을 입력하면, 자동으로 HumanMessage로 변환하여 요청\n",
    "model.invoke(\"Glory를 한국어로 번역해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. AIMessage`\n",
    "- AI 모델의 응답을 표현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI 모델의 응답 객체를 출력 \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답 객체의 자료형 확인\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 응답 텍스트 부분을 출력\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 사용량 출력\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. SystemMessage`\n",
    "- 시스템 역할에 해당 (system, developer 등)\n",
    "- AI 모델의 동작과 제약사항을 정의하는데 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage \n",
    "\n",
    "# 시스템 메시지 생성\n",
    "system_msg = SystemMessage(content=\"당신은 영어를 한국어로 번역하는 AI 어시스턴트입니다.\")\n",
    "\n",
    "# 메시지 객체 확인\n",
    "system_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 요청 (HumanMessage)과 시스템 메시지(SystemMessage)를 함께 사용\n",
    "human_message = HumanMessage(content=\"Glory\")\n",
    "messages = [system_msg, human_message]\n",
    "\n",
    "# 모델에 메시지를 보내고 응답 받기\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# 답변 출력\n",
    "print(\"답변: \", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] \n",
    "\n",
    "- Google Gemini 모델을 사용하여, 챗 메시지 목록을 기반으로 텍스트 생성하고 응답 객체를 확인해보세요.\n",
    "\n",
    "- 참고: https://python.langchain.com/docs/integrations/chat/google_generative_ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 시스템 메시지 생성\n",
    "system_msg = SystemMessage(content=\"당신은 영어를 한국어로 번역하는 AI 어시스턴트입니다.\")\n",
    "\n",
    "# 사용자 메시지 생성\n",
    "human_message = HumanMessage(content=\"Glory\")\n",
    "\n",
    "# 메시지 리스트 생성\n",
    "messages = [system_msg, human_message]\n",
    "\n",
    "# 모델에 메시지를 보내고 응답 받기\n",
    "response = gemini.invoke(messages)\n",
    "\n",
    "# 응답 객체의 메시지 내용 출력\n",
    "print(\"답변: \", response.content)\n",
    "\n",
    "# 응답 객체의 메타데이터 출력\n",
    "print(\"메타데이터: \", response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 프롬프트 템플릿 (Prompt Template)\n",
    "- 프롬프트 템플릿을 통해 일관된 입력 형식을 제공\n",
    "    1. 사용자의 입력과 파라미터를 언어 모델이 이해할 수 있는 형태로 변환하는 도구\n",
    "    2. 언어 모델에게 전달할 지시문을 만드는 틀\n",
    "- 변수를 포함한 동적 프롬프트 생성이 가능\n",
    "    1. 모든 템플릿은 딕셔너리 형태의 입력을 받아서 처리\n",
    "    2. 출력은 PromptValue 형태로 반환되며, 이는 문자열이나 메시지 리스트로 변환 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. 문자열 프롬프트 템플릿 (String PromptTemplate)`\n",
    "- 가장 기본적인 형태\n",
    "- 단일 문자열을 형식화하는데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 템플릿 생성 \n",
    "# \"{topic}에 대한 이야기를 해줘\"라는 템플릿을 사용하여\n",
    "# topic이라는 변수를 포함하는 프롬프트를 생성\n",
    "template = PromptTemplate.from_template(\"{topic}에 대한 이야기를 해줘\")\n",
    "\n",
    "# 템플릿 사용\n",
    "# \"고양이\"라는 주제를 사용하여 프롬프트 생성\n",
    "# invoke 메서드를 통해 템플릿에 값을 전달\n",
    "prompt = template.invoke({\"topic\": \"고양이\"})\n",
    "\n",
    "# 템플릿 출력\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. 채팅 프롬프트 템플릿 (ChatPromptTemplate)`\n",
    "- 여러 메시지를 포함하는 대화형 템플릿을 만들 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 채팅 템플릿 생성\n",
    "# 여기서는 시스템 메시지와 사용자 메시지를 포함하여 정의\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 도움이 되는 비서입니다\"),\n",
    "    (\"user\", \"{subject}에 대해 설명해주세요\")\n",
    "])\n",
    "\n",
    "# 템플릿 사용\n",
    "prompt = template.invoke({\"subject\": \"인공지능\"})\n",
    "\n",
    "# 출력\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. 메시지 플레이스홀더 (MessagesPlaceholder)`\n",
    "- 기존 메시지 목록을 템플릿의 특정 위치에 삽입할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# 메시지 플레이스홀더가 있는 템플릿\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 도움이 되는 비서입니다\"),\n",
    "    MessagesPlaceholder(\"chat_history\")   # 채팅 기록을 플레이스홀더로 사용 (예: 이전 대화 내용) -> 이 위치에 메시지 목록을 추가할 수 있음\n",
    "])\n",
    "\n",
    "# 템플릿 사용\n",
    "prompt = template.invoke({\n",
    "    \"chat_history\": [\n",
    "        HumanMessage(content=\"안녕하세요! 제 이름은 스티브입니다.\"),\n",
    "        AIMessage(content=\"안녕하세요! 무엇을 도와드릴까요?\"),\n",
    "        HumanMessage(content=\"제 이름을 기억하나요?\")\n",
    "        ]\n",
    "})\n",
    "\n",
    "# 출력\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] \n",
    "\n",
    "- 프롬프트 템플릿을 사용하여, 입력받은 텍스트를 요약하는 템플릿을 작성하고, Google Gemini 모델을 사용하여 요약 결과를 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 요약 요청을 위한 템플릿 생성\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 텍스트 요약을 잘하는 AI 어시스턴트입니다.\"),\n",
    "    (\"user\", \"다음 텍스트를 1~2 문장으로 핵심 내용을 위주로 간결하게 요약해주세요: {text}\")\n",
    "])\n",
    "\n",
    "# 요약 체인 생성\n",
    "summarization_chain = template | gemini\n",
    "\n",
    "# 요약할 텍스트\n",
    "text = \"\"\"\n",
    "인공지능은 컴퓨터 시스템이 인간의 지능을 모방하여 학습, 추론, 문제 해결 등을 수행하는 기술입니다.\n",
    "인공지능은 머신러닝, 딥러닝, 자연어 처리 등 다양한 분야에서 활용되며, 자율주행차, 음성 인식, 이미지 분석 등 여러 응용 프로그램에 적용됩니다.\n",
    "생성형 인공지능은 텍스트, 이미지, 음악 등 다양한 콘텐츠를 생성하는 데 사용되며, 창의적인 작업에서도 큰 역할을 하고 있습니다.\n",
    "\"\"\"\n",
    "\n",
    "# 요약 요청\n",
    "response = summarization_chain.invoke({\"text\": text})\n",
    "\n",
    "# 응답 객체의 메시지 내용 출력\n",
    "print(\"답변: \", response.content)\n",
    "\n",
    "# 응답 객체의 메타데이터 출력\n",
    "print(\"메타데이터: \", response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 출력 파서 (Output Parser)\n",
    "1. **역할과 기능**\n",
    "    - 모델의 텍스트 출력을 구조화된 데이터로 변환\n",
    "    - 채팅 모델과 LLM의 출력을 정규화\n",
    "    - 다운스트림 작업을 위한 데이터 형식 변환\n",
    "\n",
    "2. **사용 시 고려사항**\n",
    "    - OpenAI function calling과 같은 기능이 있는 경우, 해당 기능을 우선 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) StrOutputParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 기본적인 문자열 파서 사용\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 프롬프트 템플릿 설정\n",
    "prompt = PromptTemplate.from_template(\"도시 {city}의 특징을 알려주세요\")\n",
    "\n",
    "# 모델 정의\n",
    "model = ChatOpenAI(model='gpt-4.1-mini')\n",
    "\n",
    "# 체인 구성\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 체인 실행\n",
    "result = chain.invoke({\"city\": \"서울\"})\n",
    "\n",
    "# 결과 출력\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 구조화된 출력 (with_structured_output 메소드)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Pydantic 클래스로 출력 구조를 정의\n",
    "class CityInfo(BaseModel):\n",
    "    name: str = Field(description=\"도시 이름\")\n",
    "    description: str = Field(description=\"도시의 특징\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(\"도시 {city}의 특징을 알려주세요.\")\n",
    "\n",
    "# 모델 생성 및 구조화된 출력 바인딩\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "structured_model = model.with_structured_output(CityInfo)\n",
    "\n",
    "# 프롬프트와 모델 체인 연결\n",
    "chain = prompt | structured_model\n",
    "\n",
    "# 체인 실행\n",
    "result = chain.invoke({\"city\": \"서울\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 출력 (CityInfo 객체)\n",
    "print(type(result))\n",
    "print(\"-\" * 20)\n",
    "print(result)\n",
    "print(\"-\" * 20)\n",
    "print(f\"도시 이름: {result.name}\")\n",
    "print(f\"특징: {result.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] \n",
    "\n",
    "- 구조화된 출력을 사용하여, 다음 뉴스 기사에서 언론사, 기사 제목, 기사 내용, 작성자, 작성일을 추출해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 뉴스 기사를 복사하여 사용하세요.\n",
    "\n",
    "news_article = \"\"\"\n",
    "\"복잡한 납세 등 AI가 돕는다\"…정부, 초거대 AI 공공서비스 개발\n",
    "입력2025.06.20. 오후 12:00  수정2025.06.20. 오후 1:39 \n",
    "\n",
    "[서울=뉴시스]윤현성 기자 = 정부가 공공분야에 도입할 초거대 인공지능(AI) 기반 국민 편의 서비스 본격 개발에 나선다. 세금 납부 시 생성형 AI 챗봇을 통해 어려운 세무 용어 등에 대한 설명·상담을 듣거나, 대량의 민원업무도 생성형 AI 기반 분석으로 빠르게 답변·응대하는 등의 형태다.\n",
    "\n",
    "과학기술정보통신부와 디지털플랫폼정부위원회는 2025년도 '초거대 AI 서비스 개발 지원' 사업을 본격 추진하기 위해 수행기업 공모를 실시한다고 20일 밝혔다. 이 사업은 공공분야에 초거대 AI를 도입·확산하고 이를 통해 행정 효율화, 대국민 서비스 혁신, 사회현안 해결이 가능한 서비스 개발을 목표로 추진된다.\n",
    "\n",
    "올해는 다양한 공공분야에서 초거대 AI 기술을 통해 국민이 체감할 수 있고 실질적인 변화를 가져올 수 있는 과제를 중점적으로 발굴했다. 중앙부처·지자체·공공기관을 대상으로 1~2월에 과제 공모를 추진했으며 총 5개 과제가 선정됐다.\n",
    "\n",
    "국민권익위원회의 '생성형 AI 기반 국민소통·민원분석 체계 구축'은 국민소통시스템에 생성형 AI 기반 민원분석 체계를 도입해 민원처리 행정 효율화와 답변품질을 향상시킨다. 이를 통해 대량의 민원업무를 신속·효율적으로 대응해 민원업무의 효율성을 증대하고 국민의 신뢰도 향상에도 기여할 것으로 기대된다.\n",
    "\n",
    "국세청의 '생성형 AI 기반 국세 상담 지원 서비스'는 납세자가 홈택스 이용 시 전자신고 관련한 문의사항을 즉시 해소할 수 있는 실시간 상담 서비스를 제공한다. 홈택스에 상담전용 AI챗봇을 도입해 전화 상담 시 발생하는 장시간 대기 문제를 해결하고, 어려운 세무 용어 등으로 인한 불편사항을 개선할 예정이다.\n",
    "\n",
    "산업통상자원부의 '해외인증 공공특화 AI 에이전트 서비스'는 모바일 플랫폼, 소셜네트워크서비스 등 사용자 친화적인 모바일 기반 해외인증 특화 AI 에이전트 서비스를 제공한다. 중소기업이 겪는 수출 관련 애로사항인 해외 인증과 관련된 정보와 질의 응답을 AI기반으로 제공하여 손쉽게 확인할 수 있을 것으로 기대된다.\n",
    "\n",
    "국민건강보험공단의 '에이전틱 AI기반 전국민 맞춤형 민원 상담 서비스'는 국민 생활과 편익에 직결되는 건강보험 민원 상담업무에 AI를 도입해 24시간 개인 맞춤형 민원 상담 서비스를 구현한다. 기존의 전화 상담 방식의 대기 시간 문제 등을 해소하고, 고객센터 집중 상담을 분산시켜 업무의 효율성을 향상시키는데 기여할 것으로 전망이다.\n",
    "\n",
    "한국지역정보개발원의 '지방재정 지능화 서비스'는 e호조+, 지방재정365 등 지방재정서비스에 생성형 AI를 접목시켜 대국민, 공무원 등 각자의 요구에 부합하는 융복합 재정정보서비스 환경을 제공하고자 한다. 이 서비스가 도입되면 지방정부의 사회현안 해결을 위한 정책 수립의 적시성 향상 및 전문성 확보, 지자체 정보 접근성 강화로 대국민의 사회 참여 기회를 확대할 것으로 예상된다.\n",
    "\n",
    "이번 사업은 19일 국민권익위원회와 국세청 과제의 민간 전문기업 조달 공고를 시작으로, 5개 과제별 서비스 개발지원 사업이 순차적으로 입찰공고될 예정이다.\n",
    "\n",
    "김경만 과기정통부 인공지능기반정책관은 \"선정된 과제에 대해 민·관 협력을 기반으로 행정 현장의 변화와 국민이 체감할 수 있는 성과가 창출·확산될 수 있도록 적극 지원할 계획\"이라며 \"개발된 서비스는 공공분야에서 행정업무의 효율성을 높이고 대국민 서비스 품질을 높이는 데 실질적으로 기여할 수 있도록 민간에서도 많은 관심을 가져주실 것을 부탁드린다\"고 말했다.\n",
    "\n",
    "이승현 디플정위 인공지능·플랫폼혁신국장은 \"이번 사업은 노동, 복지, 민원 등 다양한 공공 분야에 AI를 도입·활용하는데 중요한 역할을 하고 있다\"며 \"올해도 AI를 활용해 사회문제를 해결하고, 대국민 서비스를 혁신적으로 개선할 수 있는 서비스가 개발되길 기대한다\"고 전했다．\n",
    "\n",
    "윤현성 기자(hsyhs@newsis.com)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skt-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
